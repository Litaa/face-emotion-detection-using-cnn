{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Face Emotion Detection using Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Seiring berkembangnya teknologi banyak penemuan diciptakan dengan menggunakan berbagai metode machine learning maupun deep learning. Salah satunya yang paling banyak digunakan adalah facial detection atau sistem pengenalan wajah. Dengan menggunakan sistem pengenalan wajah, kita dapat melakukan indentifikasi terhadap identitas bahkan emosi dari wajah yang bersangkutan. \n",
    "Kita dapat melakukan sistem pengenalan wajah secara real time dengan menggunakan web camera , cctv, ataupun melalui foto.\n",
    "Umumnya sistem pengenalan wajah ini dibangun menggunakan metode deep learning atau openCV.\n",
    "Pada notebook ini, kita akan belajar bagaimana cara mengidentifikasi emosi seseorang secara real time dengan web camera menggunakan metode deep leaning CNN (Convolutional Neural Network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "Dalam Neural Network (NN), CNN adalah model utama yang dapat digunakan untuk melakukan image recognition dan image classification. Secara teknis, CNN dipecah terlebih dahulu menjadi data train dan data test. Setiap input image yang masuk akan melalui convolutional layers kernel. Untuk detail prosesnya, dapat dilihat melalui gambar berikut ini :<br>\n",
    "<img src=\"assets/cnn.JPG\" width=\"700\" />\n",
    "\n",
    "Jika dilihat pada gambar diatas, dapat kita simpulkan bahwa CNN dibagi menjadi dua bagian besar, yaitu Feature Learning dan Classification. <br>\n",
    "\n",
    "#### Feature Learning : \n",
    "Pada tahapan Feature Learning terjadi proses ekstrasi image menjadi features angka yang merepresentasikan image tersebut, umumnya angka-angka tersebut dikemas dalam bentuk vector. Feature learning terdiri dari beberapa bagian, yaitu :<br>\n",
    "\n",
    "**1. Convolutional Layer :**<br>\n",
    "Convolutional layer terdiri dari neuron tersusun yang membentuk sebuah filter, dimana filter tersebut akan digeser keseluruh bagian dari image. Setiap pergeseran akan dilakukan operasi perkalian (dot) antara input dan filter tersebut sehingga menghasilkan output yang disebut feature map. Berikut dibawah ini adalah ilustrasi proses convolutional layer :\n",
    "![SegmentLocal](assets/cl.gif \"segment\")\n",
    "\n",
    "**2. Pooling Layers :**<br>\n",
    "Pooling layer biasanya terletak setelah convolutional layer. Pooling layer ini berfungsi untuk mengurangi overfitting dan mempercepat komputasi. Secara teknis, pooling layer akan mengurangi dimensi dari hasil output (feature map) pada convolutional layer. Pooling yang biasa digunakan adalah MaxPooling dan AveragePooling. Pada notebook ini, kita akan menggunakan MaxPooling untuk proses reduksi dimensinya.<br>\n",
    "\n",
    "**3. ReLU (Rectified Linear Unit) :**<br>\n",
    "ReLU pada proses ini berfungsi melakukan aktivasi element untuk mengurangi vanishing gradient. ReLU merupakan operasi non-linear yang memiliki fungsi Æ’(x) = max(0,x).\n",
    "\n",
    "#### Classification :\n",
    "Tahapan setelah ekstrasi fitur adalah proses klasifikasi (classification). Proses classification terdiri dari : <br>\n",
    "\n",
    "**1. Flatten :**<br>\n",
    "Flatten digunakan untuk mengubah feature map kedalam bentuk vector yang kemudian akan digunakan sebagai input pada fully-connected layer.<br>\n",
    "\n",
    "**2. Fully-connected layer :**<br>\n",
    "<img src=\"assets/fc.JPG\" width=\"600\" />\n",
    "\n",
    "Proses flatten akan menghasilkan vector yang akan dijadikan sebagai input, jika dilihat pada gambar diatas, input tersebut antara lain adalah x1,x2,x3,dst. Dengan fully-connected layer, kita akan mengkombinasikan beberapa input tersebut menjadi sebuah model yang kemudian akan dihitung bobot outputnya. <br>\n",
    "\n",
    "**3. Softmax :**<br>\n",
    "Fungsi softmax digunakan untuk menghitung probabilitas dari setiap kelas target. Probabilitas inilah yang digunakan untuk mengklasifikasikan image input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Program\n",
    "\n",
    "#### Informasi Dataset\n",
    "Dataset yang digunakan adalah dataset `fer2013.csv` yang dapat diakses pada link berikut [ini](https://www.kaggle.com/deadskull7/fer2013).<br>\n",
    "Dataset ini terdiri dari 2 kolom, yaitu `emotion` dan `pixel`, dimana pada kolom `emotion` terdapat nilai 0-6 yang bertipe categorical dan mewakili setiap emosi yang akan dideteksi.  (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). Sedangkan kolom `pixel` memuat informasi nilai pixel dari masing-masing emosi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T10:44:55.908248Z",
     "start_time": "2020-07-31T10:44:03.795666Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "0DvAnZbcDNsN",
    "outputId": "c6257f8a-ff0b-4a39-8514-ad0a9c0fb2af"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = pd.read_csv('data_input/fer2013.csv')\n",
    "\n",
    "width, height = 48, 48\n",
    "\n",
    "datapoints = data['pixels'].tolist()\n",
    "\n",
    "X = []\n",
    "for xseq in datapoints:\n",
    "    xx = [int(xp) for xp in xseq.split(' ')]\n",
    "    xx = np.asarray(xx).reshape(width, height)\n",
    "    X.append(xx.astype('float32'))\n",
    "\n",
    "X = np.asarray(X)\n",
    "X = np.expand_dims(X, -1)\n",
    "\n",
    "y = pd.get_dummies(data['emotion'])\n",
    "\n",
    "np.save('fdataX', X)\n",
    "np.save('flabels', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling CNN\n",
    "Pada proses modelling ini, pooling yang digunakan adalah MaxPooling dengan jumlah iterasi epoch sebanyak 100 kali. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3oyoXewFDN6l",
    "outputId": "0c8ce71a-cbfa-4d03-d387-d85333a603be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29068 samples, validate on 3230 samples\n",
      "Epoch 1/100\n",
      "29068/29068 [==============================] - 34s 1ms/step - loss: 1.9968 - accuracy: 0.2155 - val_loss: 1.8087 - val_accuracy: 0.2594\n",
      "Epoch 2/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.8073 - accuracy: 0.2596 - val_loss: 1.7414 - val_accuracy: 0.2994\n",
      "Epoch 3/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.7221 - accuracy: 0.3077 - val_loss: 1.5887 - val_accuracy: 0.3505\n",
      "Epoch 4/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.5990 - accuracy: 0.3674 - val_loss: 1.4473 - val_accuracy: 0.4331\n",
      "Epoch 5/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.5137 - accuracy: 0.4134 - val_loss: 1.3953 - val_accuracy: 0.4498\n",
      "Epoch 6/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.4412 - accuracy: 0.4446 - val_loss: 1.3586 - val_accuracy: 0.4734\n",
      "Epoch 7/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.3923 - accuracy: 0.4687 - val_loss: 1.3023 - val_accuracy: 0.4873\n",
      "Epoch 8/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.3560 - accuracy: 0.4831 - val_loss: 1.2854 - val_accuracy: 0.5053\n",
      "Epoch 9/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.3219 - accuracy: 0.5020 - val_loss: 1.2454 - val_accuracy: 0.5254\n",
      "Epoch 10/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.2859 - accuracy: 0.5168 - val_loss: 1.2048 - val_accuracy: 0.5368\n",
      "Epoch 11/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.2633 - accuracy: 0.5272 - val_loss: 1.2088 - val_accuracy: 0.5359\n",
      "Epoch 12/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.2416 - accuracy: 0.5342 - val_loss: 1.1690 - val_accuracy: 0.5520\n",
      "Epoch 13/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.2141 - accuracy: 0.5486 - val_loss: 1.1848 - val_accuracy: 0.5514\n",
      "Epoch 14/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.2004 - accuracy: 0.5539 - val_loss: 1.1722 - val_accuracy: 0.5632\n",
      "Epoch 15/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.1726 - accuracy: 0.5681 - val_loss: 1.1012 - val_accuracy: 0.5882\n",
      "Epoch 16/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.1510 - accuracy: 0.5779 - val_loss: 1.0992 - val_accuracy: 0.5913\n",
      "Epoch 17/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.1255 - accuracy: 0.5918 - val_loss: 1.1252 - val_accuracy: 0.5923\n",
      "Epoch 18/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.1098 - accuracy: 0.5947 - val_loss: 1.0774 - val_accuracy: 0.5957\n",
      "Epoch 19/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.0839 - accuracy: 0.6065 - val_loss: 1.0578 - val_accuracy: 0.6146\n",
      "Epoch 20/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.0584 - accuracy: 0.6178 - val_loss: 1.0553 - val_accuracy: 0.6158\n",
      "Epoch 21/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.0381 - accuracy: 0.6231 - val_loss: 1.0510 - val_accuracy: 0.6130\n",
      "Epoch 22/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.0187 - accuracy: 0.6284 - val_loss: 1.0181 - val_accuracy: 0.6149\n",
      "Epoch 23/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 1.0089 - accuracy: 0.6344 - val_loss: 1.0129 - val_accuracy: 0.6300\n",
      "Epoch 24/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.9786 - accuracy: 0.6451 - val_loss: 1.0204 - val_accuracy: 0.6266\n",
      "Epoch 25/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.9652 - accuracy: 0.6530 - val_loss: 1.0253 - val_accuracy: 0.6297\n",
      "Epoch 26/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.9408 - accuracy: 0.6596 - val_loss: 1.0279 - val_accuracy: 0.6254\n",
      "Epoch 27/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.9288 - accuracy: 0.6658 - val_loss: 1.0062 - val_accuracy: 0.6272\n",
      "Epoch 28/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.9079 - accuracy: 0.6733 - val_loss: 1.0076 - val_accuracy: 0.6359\n",
      "Epoch 29/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.8954 - accuracy: 0.6787 - val_loss: 0.9927 - val_accuracy: 0.6464\n",
      "Epoch 30/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.8733 - accuracy: 0.6863 - val_loss: 0.9962 - val_accuracy: 0.6443\n",
      "Epoch 31/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.8651 - accuracy: 0.6898 - val_loss: 1.0204 - val_accuracy: 0.6455\n",
      "Epoch 32/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.8449 - accuracy: 0.6966 - val_loss: 1.0130 - val_accuracy: 0.6477\n",
      "Epoch 33/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.8283 - accuracy: 0.7052 - val_loss: 1.0030 - val_accuracy: 0.6421\n",
      "Epoch 34/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.8197 - accuracy: 0.7073 - val_loss: 1.0299 - val_accuracy: 0.6492\n",
      "Epoch 35/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.7947 - accuracy: 0.7154 - val_loss: 1.0507 - val_accuracy: 0.6449\n",
      "Epoch 36/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.7870 - accuracy: 0.7197 - val_loss: 1.0261 - val_accuracy: 0.6402\n",
      "Epoch 37/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.7683 - accuracy: 0.7256 - val_loss: 1.0096 - val_accuracy: 0.6517\n",
      "Epoch 38/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.7454 - accuracy: 0.7331 - val_loss: 1.0281 - val_accuracy: 0.6533\n",
      "Epoch 39/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.7406 - accuracy: 0.7364 - val_loss: 1.0243 - val_accuracy: 0.6563\n",
      "Epoch 40/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.7232 - accuracy: 0.7453 - val_loss: 1.0262 - val_accuracy: 0.6548\n",
      "Epoch 41/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.7178 - accuracy: 0.7466 - val_loss: 1.0164 - val_accuracy: 0.6650\n",
      "Epoch 42/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.6954 - accuracy: 0.7532 - val_loss: 1.0662 - val_accuracy: 0.6693\n",
      "Epoch 43/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.6949 - accuracy: 0.7554 - val_loss: 1.0076 - val_accuracy: 0.6573\n",
      "Epoch 44/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.6819 - accuracy: 0.7609 - val_loss: 1.0805 - val_accuracy: 0.6563\n",
      "Epoch 45/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.6628 - accuracy: 0.7700 - val_loss: 1.0910 - val_accuracy: 0.6628\n",
      "Epoch 46/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.6567 - accuracy: 0.7705 - val_loss: 1.0378 - val_accuracy: 0.6613\n",
      "Epoch 47/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.6374 - accuracy: 0.7811 - val_loss: 1.0562 - val_accuracy: 0.6656\n",
      "Epoch 48/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.6232 - accuracy: 0.7828 - val_loss: 1.0776 - val_accuracy: 0.6616\n",
      "Epoch 49/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.6253 - accuracy: 0.7847 - val_loss: 1.0745 - val_accuracy: 0.6557\n",
      "Epoch 50/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.6074 - accuracy: 0.7885 - val_loss: 1.0793 - val_accuracy: 0.6533\n",
      "Epoch 51/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.5952 - accuracy: 0.7953 - val_loss: 1.0881 - val_accuracy: 0.6557\n",
      "Epoch 52/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.5867 - accuracy: 0.7982 - val_loss: 1.0879 - val_accuracy: 0.6638\n",
      "Epoch 53/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.5807 - accuracy: 0.8014 - val_loss: 1.0462 - val_accuracy: 0.6709\n",
      "Epoch 54/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.5621 - accuracy: 0.8082 - val_loss: 1.1073 - val_accuracy: 0.6616\n",
      "Epoch 55/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.5499 - accuracy: 0.8106 - val_loss: 1.1407 - val_accuracy: 0.6582\n",
      "Epoch 56/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.5364 - accuracy: 0.8193 - val_loss: 1.1099 - val_accuracy: 0.6622\n",
      "Epoch 57/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.5362 - accuracy: 0.8186 - val_loss: 1.1087 - val_accuracy: 0.6663\n",
      "Epoch 58/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.5324 - accuracy: 0.8218 - val_loss: 1.1970 - val_accuracy: 0.6731\n",
      "Epoch 59/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.5268 - accuracy: 0.8223 - val_loss: 1.1697 - val_accuracy: 0.6659\n",
      "Epoch 60/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.5101 - accuracy: 0.8284 - val_loss: 1.1287 - val_accuracy: 0.6638\n",
      "Epoch 61/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.5021 - accuracy: 0.8306 - val_loss: 1.1992 - val_accuracy: 0.6703\n",
      "Epoch 62/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4935 - accuracy: 0.8359 - val_loss: 1.1647 - val_accuracy: 0.6641\n",
      "Epoch 63/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4885 - accuracy: 0.8360 - val_loss: 1.1746 - val_accuracy: 0.6601\n",
      "Epoch 64/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4766 - accuracy: 0.8402 - val_loss: 1.1425 - val_accuracy: 0.6616\n",
      "Epoch 65/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4742 - accuracy: 0.8408 - val_loss: 1.1985 - val_accuracy: 0.6650\n",
      "Epoch 66/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4691 - accuracy: 0.8441 - val_loss: 1.1501 - val_accuracy: 0.6644\n",
      "Epoch 67/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4642 - accuracy: 0.8483 - val_loss: 1.2012 - val_accuracy: 0.6625\n",
      "Epoch 68/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4428 - accuracy: 0.8550 - val_loss: 1.2417 - val_accuracy: 0.6622\n",
      "Epoch 69/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4455 - accuracy: 0.8538 - val_loss: 1.1766 - val_accuracy: 0.6675\n",
      "Epoch 70/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4535 - accuracy: 0.8508 - val_loss: 1.2039 - val_accuracy: 0.6619\n",
      "Epoch 71/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4334 - accuracy: 0.8548 - val_loss: 1.2536 - val_accuracy: 0.6591\n",
      "Epoch 72/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4248 - accuracy: 0.8615 - val_loss: 1.2470 - val_accuracy: 0.6523\n",
      "Epoch 73/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4185 - accuracy: 0.8619 - val_loss: 1.2918 - val_accuracy: 0.6644\n",
      "Epoch 74/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4191 - accuracy: 0.8643 - val_loss: 1.2504 - val_accuracy: 0.6659\n",
      "Epoch 75/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4163 - accuracy: 0.8638 - val_loss: 1.2304 - val_accuracy: 0.6641\n",
      "Epoch 76/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4013 - accuracy: 0.8689 - val_loss: 1.2447 - val_accuracy: 0.6477\n",
      "Epoch 77/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3952 - accuracy: 0.8705 - val_loss: 1.2936 - val_accuracy: 0.6604\n",
      "Epoch 78/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.4025 - accuracy: 0.8682 - val_loss: 1.2479 - val_accuracy: 0.6570\n",
      "Epoch 79/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3889 - accuracy: 0.8712 - val_loss: 1.2407 - val_accuracy: 0.6576\n",
      "Epoch 80/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3780 - accuracy: 0.8800 - val_loss: 1.3246 - val_accuracy: 0.6684\n",
      "Epoch 81/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3809 - accuracy: 0.8758 - val_loss: 1.2500 - val_accuracy: 0.6703\n",
      "Epoch 82/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3686 - accuracy: 0.8829 - val_loss: 1.1964 - val_accuracy: 0.6585\n",
      "Epoch 83/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3680 - accuracy: 0.8821 - val_loss: 1.2296 - val_accuracy: 0.6678\n",
      "Epoch 84/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3575 - accuracy: 0.8846 - val_loss: 1.2927 - val_accuracy: 0.6684\n",
      "Epoch 85/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3467 - accuracy: 0.8902 - val_loss: 1.3727 - val_accuracy: 0.6743\n",
      "Epoch 86/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3456 - accuracy: 0.8896 - val_loss: 1.3187 - val_accuracy: 0.6560\n",
      "Epoch 87/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3570 - accuracy: 0.8850 - val_loss: 1.3295 - val_accuracy: 0.6585\n",
      "Epoch 88/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3421 - accuracy: 0.8930 - val_loss: 1.3542 - val_accuracy: 0.6545\n",
      "Epoch 89/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3402 - accuracy: 0.8896 - val_loss: 1.3661 - val_accuracy: 0.6545\n",
      "Epoch 90/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3274 - accuracy: 0.8942 - val_loss: 1.3779 - val_accuracy: 0.6663\n",
      "Epoch 91/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3276 - accuracy: 0.8966 - val_loss: 1.3478 - val_accuracy: 0.6613\n",
      "Epoch 92/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3259 - accuracy: 0.8970 - val_loss: 1.3176 - val_accuracy: 0.6628\n",
      "Epoch 93/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3154 - accuracy: 0.8983 - val_loss: 1.2974 - val_accuracy: 0.6659\n",
      "Epoch 94/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3272 - accuracy: 0.8974 - val_loss: 1.2920 - val_accuracy: 0.6545\n",
      "Epoch 95/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3105 - accuracy: 0.9002 - val_loss: 1.3017 - val_accuracy: 0.6625\n",
      "Epoch 96/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3172 - accuracy: 0.9018 - val_loss: 1.3880 - val_accuracy: 0.6604\n",
      "Epoch 97/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3106 - accuracy: 0.9029 - val_loss: 1.2894 - val_accuracy: 0.6588\n",
      "Epoch 98/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3067 - accuracy: 0.9051 - val_loss: 1.4007 - val_accuracy: 0.6622\n",
      "Epoch 99/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.2982 - accuracy: 0.9067 - val_loss: 1.4155 - val_accuracy: 0.6625\n",
      "Epoch 100/100\n",
      "29068/29068 [==============================] - 33s 1ms/step - loss: 0.3019 - accuracy: 0.9061 - val_loss: 1.4368 - val_accuracy: 0.6514\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "num_features = 64\n",
    "num_labels = 7\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "width, height = 48, 48\n",
    "\n",
    "x = np.load('fdataX.npy')\n",
    "y = np.load('flabels.npy')\n",
    "\n",
    "x -= np.mean(x, axis=0)\n",
    "x /= np.std(x, axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=41)\n",
    "\n",
    "np.save('modXtest', X_test)\n",
    "np.save('modytest', y_test)\n",
    "\n",
    "#model CNN\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\n",
    "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2*2*2*num_features, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2*2*num_features, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2*num_features, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.array(X_train), np.array(y_train),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(np.array(X_valid), np.array(y_valid)),\n",
    "          shuffle=True)\n",
    "\n",
    "fer_json = model.to_json()\n",
    "with open(\"model_fer_new.json\", \"w\") as json_file:\n",
    "    json_file.write(fer_json)\n",
    "model.save_weights(\"model_weight_new.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses modelling diatas menghasilkan bobot dan model yang kemudian disimpan dalam ekstensi file json dan h5.\n",
    "Jika kita melihat hasil pada proses modelling diatas, maka diperoleh informasi bahwa terjadi overfitting. Hal ini dapat dilihat dari nilai akurasi pada training set yang berkisar 90% lebih, sedangkan akurasi pada test set nya hanya berkisar 65% saja.<br>\n",
    "Kita dapat melakukan improvement pada model untuk menaikkan nilai akurasinya dengan cara menambah jumlah iterasi epoch, mengurangi nilai `stride` agar informasi yang diperoleh lebih detail, atau menambah nilai dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deteksi emosi realtime dengan menggunakan webcam\n",
    "Dibawah ini adalah kode program yang digunakan untuk melakukan deteksi wajah dan emosi secara realtime. Proses deteksi wajah menggunakan pre-train model Haar Cascade, sedangkan deteksi emosi menggunakan nilai bobot dari hasil modelling CNN diatas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T12:27:21.193490Z",
     "start_time": "2020-07-31T12:27:10.868487Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\litaimut\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\litaimut\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\litaimut\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\litaimut\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\litaimut\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\litaimut\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\litaimut\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function destroyAllWindows>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import model_from_json\n",
    "\n",
    "model = model_from_json(open(\"Model/model_fer_new.json\", \"r\").read())\n",
    "model.load_weights('Model/model_weight_new.h5')\n",
    "\n",
    "face_haar_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret,test_img=cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
    "\n",
    "    for (x,y,w,h) in faces_detected:\n",
    "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)\n",
    "        roi_gray=gray_img[y:y+w,x:x+h]\n",
    "        roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "        img_pixels = image.img_to_array(roi_gray)\n",
    "        img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "        img_pixels /= 255\n",
    "\n",
    "        predictions = model.predict(img_pixels)\n",
    "\n",
    "        max_index = np.argmax(predictions[0])\n",
    "\n",
    "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "        predicted_emotion = emotions[max_index]\n",
    "\n",
    "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "    resized_img = cv2.resize(test_img, (1000, 700))\n",
    "    cv2.imshow('Facial emotion analysis ',resized_img)\n",
    "\n",
    "\n",
    "\n",
    "    if cv2.waitKey(10) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ketika kode program tersebut diatas dijalankan, maka akan menampilkan output window dan secara otomatis webcam pada device laptop kita akan menyala. Berikut dibawah ini adalah 2 hasil deteksi emosi secara real time :\n",
    "\n",
    "**1. Senang**\n",
    "<img src=\"assets/happy.png\" width=\"400\" />\n",
    "\n",
    "**2. Netral**\n",
    "<img src=\"assets/neutral.JPG\" width=\"400\" />\n",
    "\n",
    "Hasil deteksi emosi tersebut diatas masih memiliki akurasi yang tergolong rendah, sehingga hanya beberapa emosi saja yang dapat di deteksi dengan tepat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "1. Terdapat overfitting pada saat melakukan modelling dataset `fer2013.csv` menggunakan algoritma CNN.\n",
    "2. Nilai akurasi untuk data training sebesat 90.6%, sedangkan nilai akurasi untuk data test sebesar 65%.\n",
    "3. Overfitting dapat diatasi dengan menambah jumlah iterasi epoch, mengurangi parameter `stride` agar informasi yang dihasilkan lebih detail, atau menambah parameter nilai dropout."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
